{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VRamBalla/BME590-Protein-Design/blob/main/Proj1_Final_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3SmOzLnx_Ot"
      },
      "source": [
        "# Mounting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72KfO16HInF2",
        "outputId": "68cd1ef5-e80c-4b95-ec2b-e03c180e3e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3VcIjZxXqhO"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZAYhCqUoTdj",
        "outputId": "33554bca-811e-4ff4-ef04-ea37c9cbd311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: exrex in /usr/local/lib/python3.8/dist-packages (0.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install exrex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5QMxKUTK-3F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import sklearn\n",
        "import pickle\n",
        "import exrex\n",
        "import re\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import layers\n",
        "from keras import models\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBGFGnsH3zKJ"
      },
      "source": [
        "#Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GPx2IXR3EBJ"
      },
      "source": [
        "First, the dataset is loaded in using pandas. This dataset contains protein sequence information and the 8 nucleotide PAM sequence. A dataframe called data_8nt is created, and only the protein and PAM sequences from the dataset are carried over. Additionally, the PAM sequences is split into 8 additional columns, where each column represents one nucleotide in the PAM sequence. Thus, the final dataframe we are working with contains 11 columns: 1 for the amino acid sequence, 1 for the PAM sequence, 8 for each nucleotide position, and 1 for the amino acid sequence length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BijN3veCk830"
      },
      "source": [
        "Below is the code used to generate the final cleaned data set that was used in model training. The code is commented out because it only needs to be run once since the cleaned data was written as a csv, so it can now just be loaded in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25lqtecKzQRP"
      },
      "outputs": [],
      "source": [
        "# data8 = pd.read_csv('/content/drive/My Drive/team_5/project_1/Data/PAM_data8_ranked.csv')\n",
        "# casper_data = pd.read_csv('/content/drive/My Drive/team_5/project_1/Data/additional_PAM.csv')\n",
        "# casper_data = casper_data.rename(columns={\"Amino Acid Sequence\": \"AA Sequence\"})\n",
        "# data_8nt = pd.DataFrame()\n",
        "# data_8nt['AA Sequence'] = data8['Sequence']\n",
        "# data_8nt['PAM'] = data8['consensus PAM']\n",
        "# data_8nt = data_8nt.append(casper_data)\n",
        "\n",
        "# l = []\n",
        "# for i in data_8nt['AA Sequence']:\n",
        "#   l.append(len(i))\n",
        "  \n",
        "# data_8nt['Sequence Length'] = l\n",
        "\n",
        "# data_8nt['nt1'] = data_8nt.PAM.str.split('',expand=True)[1]\n",
        "# data_8nt['nt2'] = data_8nt.PAM.str.split('',expand=True)[2]\n",
        "# data_8nt['nt3'] = data_8nt.PAM.str.split('',expand=True)[3]\n",
        "# data_8nt['nt4'] = data_8nt.PAM.str.split('',expand=True)[4]\n",
        "# data_8nt['nt5'] = data_8nt.PAM.str.split('',expand=True)[5]\n",
        "# data_8nt['nt6'] = data_8nt.PAM.str.split('',expand=True)[6]\n",
        "# data_8nt['nt7'] = data_8nt.PAM.str.split('',expand=True)[7]\n",
        "# data_8nt['nt8'] = data_8nt.PAM.str.split('',expand=True)[8]\n",
        "\n",
        "# data_8nt.to_csv('/content/drive/My Drive/team_5/project_1/Data/Final_Cleaned_Dataset.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHUa5rYww0da"
      },
      "source": [
        "The data to be used in this project has been cleaned and exported as a csv named 'Final_Cleaned_Dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egUdD6y5lP9v"
      },
      "outputs": [],
      "source": [
        "data_8nt = pd.read_csv('/content/drive/My Drive/BME590 - Deep Learning for Protein Design - Spring 2023/Teams/team_5/project_1/Final_Cleaned_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLhXYrXcy1IJ"
      },
      "outputs": [],
      "source": [
        "protseqs = list(data_8nt['AA Sequence'])\n",
        "seq_len = list(data_8nt['Sequence Length'])\n",
        "max_seq_len = np.max(seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh9DezJ5f-Bu"
      },
      "source": [
        "#Protein Embedding\n",
        "Below are the various embedding algorithms we implemented and played around with. **We ultimately decided to only use the VHSE embedding** as it contains information about features (such as hydrophobicity, sterics and electronic properties) that we felt are most important to Cas9 binding to DNA sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wIOpbNMDPyq"
      },
      "source": [
        "## VHSE Embedding \n",
        "This function takes in an amino acid sequence, splits the string into its component characters, and assembles a VHSE encoded matrix. Each row of the matrix represents one amino acid in the protein sequence, and there are 8 columns for each of the 8 VHSE values. The function also takes as an input the maximum length of a protein sequence so that adequate zero-padding can be applied to make sure all inputs are of the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkvmgVOlDO_Q"
      },
      "outputs": [],
      "source": [
        "#Using pandas to read in the csv file containing the VHSE encoding values for each amino acid\n",
        "VHSE_val = pd.read_csv('/content/drive/My Drive/BME590 - Deep Learning for Protein Design - Spring 2023/Teams/team_5/project_1/VHSE8.csv')\n",
        "\n",
        "def VHSE_encode(seq, max_len):\n",
        "    s = [*seq]\n",
        "    m = []\n",
        "    for i in range(0,len(s)):\n",
        "        for l in range(0,np.shape(VHSE_val)[0]):\n",
        "            if s[i] == VHSE_val['Single Code'][l]:\n",
        "                m.append(VHSE_val.iloc[l][2:10]) #This adds all 8 VHSE values for a given amino acid\n",
        "    \n",
        "    if len(m) != max_len:\n",
        "       for x in range(0,max_len-len(m)):\n",
        "         m.append([0,0,0,0,0,0,0,0])\n",
        "\n",
        "    a = np.array(m).astype('float32')\n",
        "\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isWXG4vB2Pk5"
      },
      "source": [
        "The VHSE takes approximately 30 minutes to run due to the large volume of data. Thus, instead of having to re-embed the protein sequences every time we start a new runtime, the VHSE encoded protein sequences were saved as a csv file that could then be loaded and reshaped into the 3D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciJKJvoLSLUZ"
      },
      "outputs": [],
      "source": [
        "#VHSE_encoded = [VHSE_encode(seq,max_seq_len) for seq in protseqs]\n",
        "#VHSE_encoded = np.array(VHSE_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpYnGUgC2qKF"
      },
      "source": [
        "The two functions defined below allow for the export and import of the VHSE-embedded csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acVK80E3Anj6"
      },
      "outputs": [],
      "source": [
        "def export_matrix(a,file):\n",
        "    a_reshaped = a.reshape(a.shape[0],-1)\n",
        "    np.savetxt(file,a_reshaped)\n",
        "\n",
        "def import_matrix(file,twoshape):\n",
        "  loaded_a = np.loadtxt(file)\n",
        "  load_original_a = loaded_a.reshape(loaded_a.shape[0],loaded_a.shape[1] // twoshape, twoshape)\n",
        "  return load_original_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B0792CP21PR"
      },
      "source": [
        "This code block has been commented because the csv export has already happened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XikkjwIMyw-"
      },
      "outputs": [],
      "source": [
        " #export_matrix(VHSE_encoded,'/content/drive/My Drive/team_5/project_1/Data/FinalData_VHSE_Encoded.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxfjkjIl8Ksn"
      },
      "source": [
        "The final embedding result is a 3D array, where each sample is encoded as a 1679 x 8 matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOJBnBZKC4M-"
      },
      "outputs": [],
      "source": [
        "VHSE_encoded = import_matrix('/content/drive/My Drive/BME590 - Deep Learning for Protein Design - Spring 2023/Teams/team_5/project_1/FinalData_VHSE_Encoded.csv',8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuzYE4_hfBQ9"
      },
      "source": [
        "## Bag of Words Embedding\n",
        "This encoding involves assigning each amino acid a number, so that a protein sequence can be translated into set of values where each number ranges from 1-20 (since there are 20 amino acids). Thus, the final result of this embedding is a 2D array of size n x l, where n is the number of samples, and l the maximum amino acid sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1RXWD1bmoi2",
        "outputId": "fb2ae770-8139-41de-b86a-22ad3e2e254a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "amino_acids = [' ', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
        "               'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "len(amino_acids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nd7ZN2jsdpl"
      },
      "outputs": [],
      "source": [
        "def BoW_encoding(seq, max_len):\n",
        "  encoding = []\n",
        "  for aa in seq:\n",
        "    if aa in amino_acids:\n",
        "      encoding.append(amino_acids.index(aa))\n",
        "  while(len(encoding) != max_len):\n",
        "    encoding.append(0)\n",
        "  return np.array(encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e7MxVZp-2K2"
      },
      "outputs": [],
      "source": [
        "BoW_seq = [BoW_encoding(seq,max_seq_len) for seq in protseqs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BZtV6xfIvaY"
      },
      "source": [
        "## BLOSUM62 Embedding\n",
        "This encoding is implemented in a manner similar to the VHSE embedding. However, the values for each amino acid in BLOSUM62 represent the evolutionary distance of the amino acid to every other. The final result is a 3D array of size 20 x l, where 20 represents the 20 blosum values for each amino acid and l is the maximum protein sequence length in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ErYsBg1qZV"
      },
      "outputs": [],
      "source": [
        "blosum62 = pd.read_csv('/content/drive/My Drive/team_5/project_1/Unclean Data/BLOSUM62.csv', index_col=0)\n",
        "blosum62 = blosum62.drop(['B','X','Z','*'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI71_NgpIwfl"
      },
      "outputs": [],
      "source": [
        "#encode a peptide into blosum features\n",
        "def blosum_encode(seq):\n",
        "    s = [*seq]\n",
        "    x = pd.DataFrame()\n",
        "    for i in range(0,len(s)):\n",
        "      for l in range(0,np.shape(blosum62)[1]):\n",
        "        if s[i] == blosum62.columns[l]:\n",
        "          x[i] = blosum62.iloc[l]\n",
        "\n",
        "    if len(x.columns) != max_seq_len:\n",
        "       for f in range(0,max_seq_len-len(x.columns)):\n",
        "         x.loc[len(x)] = 0 # 0 pad for columns\n",
        "\n",
        "   \n",
        "    d = np.array(x).astype('float32')\n",
        "    print(d)\n",
        "    e = x.values.flatten()  \n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYCdQvGaJGGu"
      },
      "outputs": [],
      "source": [
        "blosum_protseq = [blosum_encode(seq) for seq in protseqs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fMk0IDSfUVN"
      },
      "source": [
        "# Model Training\n",
        "Below are the models we tried to use. We played around with a Convolutional Neural Network, Support Vector Machines, Logisitic Regressions, and a Boosted Decision Tree. Ultimately all of these models were able to predict a particular nucleotide in the PAM sequence with decent accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eUehJB_REUZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO3CPM3aREBl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mX0Le4qh4Sv"
      },
      "source": [
        "## CNN\n",
        "8 CNNs using the same model architecture were run: each time the VHSE-embedded protein inputs reamined the same, but the 2D PAM labels were swapped out for each run of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_R0yyw3E46"
      },
      "source": [
        "###Data Preparation for CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twa0H-1nyXih"
      },
      "source": [
        "#### PAM Encoding for CNN\n",
        "This section contains the code by which PAM sequences were encoded in n x 4 matrices for use as a multi-label output in our 8 CNN models (1 model for each nucleotide position)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwviNC0JzN3l"
      },
      "source": [
        "The below dictionary assigns each of the base 4 nucleotides to a number; this is the column index number that will later be used when constructing the PAM encoded arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP0LiWA8Vqr9"
      },
      "outputs": [],
      "source": [
        "nt_dict = {'A':0,'C':1,'T':2,'G':3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15PSAulZ1KcH"
      },
      "outputs": [],
      "source": [
        "#Create list for each nucleotide in PAM sequence\n",
        "nt1 = data_8nt['nt1'].tolist()\n",
        "nt2 = data_8nt['nt2'].tolist()\n",
        "nt3 = data_8nt['nt3'].tolist()\n",
        "nt4 = data_8nt['nt4'].tolist()\n",
        "nt5 = data_8nt['nt5'].tolist()\n",
        "nt6 = data_8nt['nt6'].tolist()\n",
        "nt7 = data_8nt['nt7'].tolist()\n",
        "nt8 = data_8nt['nt8'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollgBtbYxSdR"
      },
      "source": [
        "The dictionary below assigns all of the IUPAC ambiguity codes to their meanings in terms of the base 4 nucleotides. Thus, coupling this with the expand_PAM function defined below outputs a list with all of the possible DNA sequences only in terms of A,C,T,G even if the input sequence contains an ambiguity code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6nB1opKoSYc"
      },
      "outputs": [],
      "source": [
        "IUPAC_r = {\n",
        "    'N': '(A|C|T|G)',\n",
        "    'R': '(G|A)',\n",
        "    'Y': '(T|C)',\n",
        "    'M': '(A|C)',\n",
        "    'K': '(G|T)',\n",
        "    'S': '(G|C)',\n",
        "    'W': '(A|T)',\n",
        "    'H': '(A|C|T)',\n",
        "    'B': '(G|C|T)',\n",
        "    'V': '(A|C|G)',\n",
        "    'D': '(A|G|T)'\n",
        "}\n",
        "\n",
        "ambiguity_codes = list(IUPAC_r.keys())\n",
        "\n",
        "def expand_PAM(seq):\n",
        "  for s in seq:\n",
        "    if s in IUPAC_r:\n",
        "      seq = seq.replace(s, IUPAC_r[s])\n",
        "  seq = list(exrex.generate(seq))\n",
        "  return seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfGYcreCzh_p"
      },
      "source": [
        "The below function constructs the 2D PAM encoding array for each nucleotide position in the PAM sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYoxX0d3sMZZ"
      },
      "outputs": [],
      "source": [
        "def nt_encode(ntlist):\n",
        "  a = np.zeros((len(ntlist),4))\n",
        "  for i in range(0,len(ntlist)):\n",
        "    if type(ntlist[i]) == str:\n",
        "      if ntlist[i] in ambiguity_codes:\n",
        "        e = expand_PAM(ntlist[i])\n",
        "        for x in e:\n",
        "         a[i][nt_dict[x]] = 1\n",
        "      else:\n",
        "        a[i][nt_dict[ntlist[i]]] = 1\n",
        "\n",
        "  return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYjYKSpPsPkW"
      },
      "outputs": [],
      "source": [
        "#Generate 2D PAM encoding arrays for each nucleotide position in the PAM sequence\n",
        "nt1_encoded = nt_encode(nt1)\n",
        "nt2_encoded = nt_encode(nt2)\n",
        "nt3_encoded = nt_encode(nt3)\n",
        "nt4_encoded = nt_encode(nt4)\n",
        "nt5_encoded = nt_encode(nt5)\n",
        "nt6_encoded = nt_encode(nt6)\n",
        "nt7_encoded = nt_encode(nt7)\n",
        "nt8_encoded = nt_encode(nt8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbbDA64N30cH"
      },
      "source": [
        "####Reshaping the VHSE Protein Embedding\n",
        "The VHSE encoded array is reshaped so that it can be a tensor, which is the required input for the CNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxzrQf7O3H4Y"
      },
      "outputs": [],
      "source": [
        "VHSE_encoded_tensor = np.expand_dims(VHSE_encoded,-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gccE3uQn43T-"
      },
      "source": [
        "####Splitting the Data for CNN\n",
        "The data for each nucleotide position was split as 72/8/20 for train/val/test. The VHSE-encoded protein sequences remain constant across all models, only the PAM nucleotide position labels are changing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "YzSxw3w37RnR",
        "outputId": "e2a11831-01cc-44b9-a95f-4674f3f2f2a7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-584da0ae0987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVHSE_encoded_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnt1_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_nt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVHSE_encoded_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnt2_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_nt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VHSE_encoded_tensor' is not defined"
          ]
        }
      ],
      "source": [
        "train_seq, test_seq, train_nt1, test_nt1 = train_test_split(VHSE_encoded_tensor,nt1_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt1, val_nt1 = train_test_split(train_seq, train_nt1, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt2, test_nt2 = train_test_split(VHSE_encoded_tensor,nt2_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt2, val_nt2 = train_test_split(train_seq, train_nt2, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt3, test_nt3 = train_test_split(VHSE_encoded_tensor,nt3_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt3, val_nt3 = train_test_split(train_seq, train_nt3, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt4, test_nt4 = train_test_split(VHSE_encoded_tensor,nt4_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt4, val_nt4 = train_test_split(train_seq, train_nt4, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt5, test_nt5 = train_test_split(VHSE_encoded_tensor,nt5_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt5, val_nt5 = train_test_split(train_seq, train_nt5, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt6, test_nt6 = train_test_split(VHSE_encoded_tensor,nt6_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt6, val_nt6 = train_test_split(train_seq, train_nt6, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt7, test_nt7 = train_test_split(VHSE_encoded_tensor,nt7_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt7, val_nt7 = train_test_split(train_seq, train_nt7, test_size=0.1, random_state=42)\n",
        "\n",
        "train_seq, test_seq, train_nt8, test_nt8 = train_test_split(VHSE_encoded_tensor,nt8_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt8, val_nt8 = train_test_split(train_seq, train_nt8, test_size=0.1, random_state=42)\n",
        "\n",
        "print(np.shape(train_seq))\n",
        "print(np.shape(train_nt1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmLxQda558bh"
      },
      "source": [
        "###CNN Model Architecture\n",
        "The CNN model architecutre consists of a convolution layer with 32 kernels of size (3,3), followed by a 2x2 maxpool layer, followed by another convolution layer with 64 kernels of size (3,3), followed by another 2x2 maxpool layer, followed by a flatten layer, dropout of 0.6, and a final dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJeXCETM7732",
        "outputId": "0f9e8bb5-17e8-4577-c171-58663ca3a090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 1697, 8, 32)       320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 849, 4, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 849, 4, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 425, 2, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 54400)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 54400)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 217604    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 236,420\n",
            "Trainable params: 236,420\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shape = (1697,8,1)\n",
        "num_classes = 4\n",
        "\n",
        "# CNN for 1st Nucleotide Position\n",
        "\n",
        "model = keras.Sequential(\n",
        "    \n",
        "    [\n",
        "        keras.Input(shape = input_shape),\n",
        "        layers.Conv2D(32, kernel_size = (3,3), padding='same',activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size = (2, 2),padding='same'),\n",
        "        layers.Conv2D(64, kernel_size = (3,3), padding='same',activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size = (2, 2),padding='same'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.6),\n",
        "        layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "model.compile(optimizer='adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcUweA1e6DLF"
      },
      "source": [
        "###Running the CNN Models\n",
        "A function was defined so that all 8 CNN models could be run with relative ease. The outputs display the loss and accuracy for the training and validation sets during training, and the test loss and accuracy during the model test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYVQPMqP9wVO"
      },
      "outputs": [],
      "source": [
        "def CNN(model,train_input,train_labels,val_input,val_labels,test_input,test_labels):\n",
        "  history = model.fit(x= train_input,\n",
        "                      y= train_labels,\n",
        "                      batch_size=60, \n",
        "                      callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)],\n",
        "                      validation_data = (val_input,val_labels),\n",
        "                      verbose = 1,\n",
        "                      epochs = 50)\n",
        "  \n",
        "  score = model.evaluate(x = test_input, \n",
        "                             y = test_labels,\n",
        "                             verbose=1)\n",
        "  \n",
        "  print('Test loss:', score[0]) \n",
        "  print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kIWZq7ecdWe"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt1,val_seq,val_nt1,test_seq,test_nt1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2RSiEBNx3mP"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt2,val_seq,val_nt2,test_seq,test_nt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFFmJs7XyBNh"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt3,val_seq,val_nt3,test_seq,test_nt3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oWFD_jRyCvD"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt4,val_seq,val_nt4,test_seq,test_nt4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98lYCh5AyFSb",
        "outputId": "fa8fbaa9-dbcb-4e02-ac9c-49f28401798b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "76/76 [==============================] - 115s 2s/step - loss: 0.6594 - accuracy: 0.6364 - val_loss: 0.5651 - val_accuracy: 0.7405\n",
            "Epoch 2/50\n",
            "76/76 [==============================] - 112s 1s/step - loss: 0.5537 - accuracy: 0.6948 - val_loss: 0.5180 - val_accuracy: 0.7764\n",
            "Epoch 3/50\n",
            "76/76 [==============================] - 115s 2s/step - loss: 0.5215 - accuracy: 0.6930 - val_loss: 0.4928 - val_accuracy: 0.7585\n",
            "Epoch 4/50\n",
            "76/76 [==============================] - 114s 2s/step - loss: 0.5036 - accuracy: 0.7103 - val_loss: 0.4838 - val_accuracy: 0.7046\n",
            "Epoch 5/50\n",
            "76/76 [==============================] - 112s 1s/step - loss: 0.4921 - accuracy: 0.6861 - val_loss: 0.4703 - val_accuracy: 0.7904\n",
            "Epoch 6/50\n",
            "76/76 [==============================] - 114s 1s/step - loss: 0.4797 - accuracy: 0.6985 - val_loss: 0.4628 - val_accuracy: 0.7206\n",
            "Epoch 7/50\n",
            "29/76 [==========>...................] - ETA: 1:08 - loss: 0.4714 - accuracy: 0.6868"
          ]
        }
      ],
      "source": [
        "CNN(model,train_seq,train_nt5,val_seq,val_nt5,test_seq,test_nt5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WFFE_tRyKX9"
      },
      "outputs": [],
      "source": [
        "CNN(model,train_seq,train_nt6,val_seq,val_nt6,test_seq,test_nt6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1V9umOBVyNEg",
        "outputId": "119e6657-45f2-42b0-8562-32a777e615b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.6692 - accuracy: 0.3869 - val_loss: 0.6065 - val_accuracy: 0.5210\n",
            "Epoch 2/50\n",
            "76/76 [==============================] - 114s 1s/step - loss: 0.6004 - accuracy: 0.4698 - val_loss: 0.5749 - val_accuracy: 0.5868\n",
            "Epoch 3/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5830 - accuracy: 0.5149 - val_loss: 0.5639 - val_accuracy: 0.5768\n",
            "Epoch 4/50\n",
            "76/76 [==============================] - 116s 2s/step - loss: 0.5681 - accuracy: 0.5184 - val_loss: 0.5490 - val_accuracy: 0.6008\n",
            "Epoch 5/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5585 - accuracy: 0.5271 - val_loss: 0.5447 - val_accuracy: 0.5808\n",
            "Epoch 6/50\n",
            "76/76 [==============================] - 112s 1s/step - loss: 0.5485 - accuracy: 0.5264 - val_loss: 0.5350 - val_accuracy: 0.6148\n",
            "Epoch 7/50\n",
            "76/76 [==============================] - 115s 2s/step - loss: 0.5417 - accuracy: 0.5253 - val_loss: 0.5340 - val_accuracy: 0.5828\n",
            "Epoch 8/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5372 - accuracy: 0.5206 - val_loss: 0.5289 - val_accuracy: 0.5709\n",
            "Epoch 9/50\n",
            "76/76 [==============================] - 112s 1s/step - loss: 0.5311 - accuracy: 0.5430 - val_loss: 0.5261 - val_accuracy: 0.5768\n",
            "Epoch 10/50\n",
            "76/76 [==============================] - 114s 1s/step - loss: 0.5272 - accuracy: 0.5357 - val_loss: 0.5212 - val_accuracy: 0.5928\n",
            "Epoch 11/50\n",
            "76/76 [==============================] - 112s 1s/step - loss: 0.5224 - accuracy: 0.5328 - val_loss: 0.5293 - val_accuracy: 0.5788\n",
            "Epoch 12/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5223 - accuracy: 0.5286 - val_loss: 0.5188 - val_accuracy: 0.5749\n",
            "Epoch 13/50\n",
            "76/76 [==============================] - 114s 1s/step - loss: 0.5135 - accuracy: 0.5337 - val_loss: 0.5145 - val_accuracy: 0.5649\n",
            "Epoch 14/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5136 - accuracy: 0.5313 - val_loss: 0.5138 - val_accuracy: 0.5848\n",
            "Epoch 15/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5097 - accuracy: 0.5397 - val_loss: 0.5148 - val_accuracy: 0.5888\n",
            "Epoch 16/50\n",
            "76/76 [==============================] - 112s 1s/step - loss: 0.5110 - accuracy: 0.5253 - val_loss: 0.5137 - val_accuracy: 0.5768\n",
            "Epoch 17/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5032 - accuracy: 0.5499 - val_loss: 0.5121 - val_accuracy: 0.6028\n",
            "Epoch 18/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5014 - accuracy: 0.5444 - val_loss: 0.5105 - val_accuracy: 0.5928\n",
            "Epoch 19/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.4991 - accuracy: 0.5317 - val_loss: 0.5117 - val_accuracy: 0.5509\n",
            "Epoch 20/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5020 - accuracy: 0.5441 - val_loss: 0.5112 - val_accuracy: 0.5589\n",
            "Epoch 21/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.4962 - accuracy: 0.5424 - val_loss: 0.5090 - val_accuracy: 0.5908\n",
            "Epoch 22/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4899 - accuracy: 0.5439 - val_loss: 0.5090 - val_accuracy: 0.5948\n",
            "Epoch 23/50\n",
            "76/76 [==============================] - 117s 2s/step - loss: 0.4952 - accuracy: 0.5492 - val_loss: 0.5090 - val_accuracy: 0.5589\n",
            "Epoch 24/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.4886 - accuracy: 0.5537 - val_loss: 0.5105 - val_accuracy: 0.5529\n",
            "Epoch 25/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4906 - accuracy: 0.5424 - val_loss: 0.5064 - val_accuracy: 0.5409\n",
            "Epoch 26/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4859 - accuracy: 0.5268 - val_loss: 0.5061 - val_accuracy: 0.5489\n",
            "Epoch 27/50\n",
            "76/76 [==============================] - 110s 1s/step - loss: 0.4837 - accuracy: 0.5490 - val_loss: 0.5062 - val_accuracy: 0.5828\n",
            "Epoch 28/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.4818 - accuracy: 0.5308 - val_loss: 0.5095 - val_accuracy: 0.5469\n",
            "Epoch 29/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.4809 - accuracy: 0.5306 - val_loss: 0.5041 - val_accuracy: 0.5908\n",
            "Epoch 30/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4769 - accuracy: 0.5490 - val_loss: 0.5043 - val_accuracy: 0.5649\n",
            "Epoch 31/50\n",
            "76/76 [==============================] - 110s 1s/step - loss: 0.4737 - accuracy: 0.5446 - val_loss: 0.5046 - val_accuracy: 0.5729\n",
            "Epoch 32/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.4752 - accuracy: 0.5435 - val_loss: 0.5051 - val_accuracy: 0.5988\n",
            "Epoch 33/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.4746 - accuracy: 0.5512 - val_loss: 0.5068 - val_accuracy: 0.5908\n",
            "Epoch 34/50\n",
            "76/76 [==============================] - 105s 1s/step - loss: 0.4744 - accuracy: 0.5586 - val_loss: 0.5052 - val_accuracy: 0.5589\n",
            "40/40 [==============================] - 9s 222ms/step - loss: 0.5007 - accuracy: 0.4749\n",
            "Test loss: 0.5007261633872986\n",
            "Test accuracy: 0.4748603403568268\n"
          ]
        }
      ],
      "source": [
        "CNN(model,train_seq,train_nt7,val_seq,val_nt7,test_seq,test_nt7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cT_IwWO4yQHs",
        "outputId": "cc4f59ba-1f25-4326-b23b-a460f22af2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.6820 - accuracy: 0.3321 - val_loss: 0.6311 - val_accuracy: 0.3693\n",
            "Epoch 2/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.6155 - accuracy: 0.4028 - val_loss: 0.5541 - val_accuracy: 0.5269\n",
            "Epoch 3/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5883 - accuracy: 0.4627 - val_loss: 0.5448 - val_accuracy: 0.5329\n",
            "Epoch 4/50\n",
            "76/76 [==============================] - 110s 1s/step - loss: 0.5652 - accuracy: 0.4984 - val_loss: 0.5406 - val_accuracy: 0.5250\n",
            "Epoch 5/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5567 - accuracy: 0.5060 - val_loss: 0.5320 - val_accuracy: 0.5329\n",
            "Epoch 6/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5582 - accuracy: 0.5049 - val_loss: 0.5263 - val_accuracy: 0.5489\n",
            "Epoch 7/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.5526 - accuracy: 0.5197 - val_loss: 0.5313 - val_accuracy: 0.5349\n",
            "Epoch 8/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5472 - accuracy: 0.5217 - val_loss: 0.5256 - val_accuracy: 0.5469\n",
            "Epoch 9/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5420 - accuracy: 0.5149 - val_loss: 0.5191 - val_accuracy: 0.5349\n",
            "Epoch 10/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5356 - accuracy: 0.5149 - val_loss: 0.5164 - val_accuracy: 0.5389\n",
            "Epoch 11/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5350 - accuracy: 0.5271 - val_loss: 0.5163 - val_accuracy: 0.5489\n",
            "Epoch 12/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5344 - accuracy: 0.5326 - val_loss: 0.5149 - val_accuracy: 0.5309\n",
            "Epoch 13/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5295 - accuracy: 0.5215 - val_loss: 0.5141 - val_accuracy: 0.5429\n",
            "Epoch 14/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5287 - accuracy: 0.5421 - val_loss: 0.5113 - val_accuracy: 0.5429\n",
            "Epoch 15/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5267 - accuracy: 0.5197 - val_loss: 0.5107 - val_accuracy: 0.5389\n",
            "Epoch 16/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5198 - accuracy: 0.5342 - val_loss: 0.5100 - val_accuracy: 0.5469\n",
            "Epoch 17/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5226 - accuracy: 0.5344 - val_loss: 0.5093 - val_accuracy: 0.5269\n",
            "Epoch 18/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5209 - accuracy: 0.5253 - val_loss: 0.5068 - val_accuracy: 0.5788\n",
            "Epoch 19/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5117 - accuracy: 0.5481 - val_loss: 0.5099 - val_accuracy: 0.5489\n",
            "Epoch 20/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.5153 - accuracy: 0.5415 - val_loss: 0.5125 - val_accuracy: 0.5709\n",
            "Epoch 21/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5128 - accuracy: 0.5377 - val_loss: 0.5068 - val_accuracy: 0.5469\n",
            "Epoch 22/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.5127 - accuracy: 0.5268 - val_loss: 0.5084 - val_accuracy: 0.5429\n",
            "Epoch 23/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5039 - accuracy: 0.5444 - val_loss: 0.5073 - val_accuracy: 0.5549\n",
            "Epoch 24/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.5070 - accuracy: 0.5588 - val_loss: 0.5028 - val_accuracy: 0.5669\n",
            "Epoch 25/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5043 - accuracy: 0.5486 - val_loss: 0.5051 - val_accuracy: 0.5529\n",
            "Epoch 26/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.5075 - accuracy: 0.5373 - val_loss: 0.5048 - val_accuracy: 0.5629\n",
            "Epoch 27/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.5027 - accuracy: 0.5504 - val_loss: 0.5047 - val_accuracy: 0.5709\n",
            "Epoch 28/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.4983 - accuracy: 0.5577 - val_loss: 0.5028 - val_accuracy: 0.5429\n",
            "Epoch 29/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4969 - accuracy: 0.5550 - val_loss: 0.5045 - val_accuracy: 0.5549\n",
            "Epoch 30/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4972 - accuracy: 0.5575 - val_loss: 0.5041 - val_accuracy: 0.5968\n",
            "Epoch 31/50\n",
            "76/76 [==============================] - 110s 1s/step - loss: 0.4925 - accuracy: 0.5439 - val_loss: 0.5026 - val_accuracy: 0.5609\n",
            "Epoch 32/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.4950 - accuracy: 0.5592 - val_loss: 0.5083 - val_accuracy: 0.5908\n",
            "Epoch 33/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4947 - accuracy: 0.5586 - val_loss: 0.5025 - val_accuracy: 0.5549\n",
            "Epoch 34/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.4932 - accuracy: 0.5614 - val_loss: 0.5039 - val_accuracy: 0.5828\n",
            "Epoch 35/50\n",
            "76/76 [==============================] - 110s 1s/step - loss: 0.4949 - accuracy: 0.5543 - val_loss: 0.5011 - val_accuracy: 0.6028\n",
            "Epoch 36/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4899 - accuracy: 0.5614 - val_loss: 0.5046 - val_accuracy: 0.5848\n",
            "Epoch 37/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4879 - accuracy: 0.5539 - val_loss: 0.5044 - val_accuracy: 0.6128\n",
            "Epoch 38/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.4875 - accuracy: 0.5583 - val_loss: 0.5067 - val_accuracy: 0.6068\n",
            "Epoch 39/50\n",
            "76/76 [==============================] - 111s 1s/step - loss: 0.4893 - accuracy: 0.5661 - val_loss: 0.5007 - val_accuracy: 0.5828\n",
            "Epoch 40/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.4818 - accuracy: 0.5668 - val_loss: 0.5040 - val_accuracy: 0.5888\n",
            "Epoch 41/50\n",
            "76/76 [==============================] - 108s 1s/step - loss: 0.4804 - accuracy: 0.5659 - val_loss: 0.5022 - val_accuracy: 0.6128\n",
            "Epoch 42/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4804 - accuracy: 0.5659 - val_loss: 0.5024 - val_accuracy: 0.5988\n",
            "Epoch 43/50\n",
            "76/76 [==============================] - 109s 1s/step - loss: 0.4753 - accuracy: 0.5654 - val_loss: 0.5019 - val_accuracy: 0.5908\n",
            "Epoch 44/50\n",
            "76/76 [==============================] - 107s 1s/step - loss: 0.4773 - accuracy: 0.5603 - val_loss: 0.5014 - val_accuracy: 0.5888\n",
            "40/40 [==============================] - 8s 212ms/step - loss: 0.5135 - accuracy: 0.5850\n",
            "Test loss: 0.5135381817817688\n",
            "Test accuracy: 0.5849959850311279\n"
          ]
        }
      ],
      "source": [
        "CNN(model,train_seq,train_nt8,val_seq,val_nt8,test_seq,test_nt8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c9PE8yO0FfJ"
      },
      "source": [
        "##Reshaping The Data For SVM, Logistic Regression, and AdaBoost\n",
        "These models cannot handle inputs with dimension greater than 2; thus the VHSE-embedded protein data was flattened into a 2D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "qV6zYCY_s-RF",
        "outputId": "8cd540ed-b850-41be-d0f1-63f853c3556b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6cfcfe15-ae0b-4dc7-920b-ee909c0b5987\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>13566</th>\n",
              "      <th>13567</th>\n",
              "      <th>13568</th>\n",
              "      <th>13569</th>\n",
              "      <th>13570</th>\n",
              "      <th>13571</th>\n",
              "      <th>13572</th>\n",
              "      <th>13573</th>\n",
              "      <th>13574</th>\n",
              "      <th>13575</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>1.01</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.61</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>1.27</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-0.67</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>1.27</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>0.76</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.76</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6257</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>0.61</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>1.27</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6258</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1.01</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6259</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>0.61</td>\n",
              "      <td>-0.67</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6260</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>1.52</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6261</th>\n",
              "      <td>1.01</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.36</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>1.52</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>1.27</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>1.27</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6262 rows  13576 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6cfcfe15-ae0b-4dc7-920b-ee909c0b5987')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6cfcfe15-ae0b-4dc7-920b-ee909c0b5987 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6cfcfe15-ae0b-4dc7-920b-ee909c0b5987');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      0      1      2      3      4      5      6      7      8      9      \\\n",
              "0      1.01  -1.17  -0.99   1.01  -1.17  -0.99   0.22   0.61  -0.34   1.27   \n",
              "1      1.01  -0.67  -1.15   1.36   0.76   1.36  -0.20   1.36  -1.15   1.27   \n",
              "2      1.01  -0.20   0.61  -0.34   0.76  -0.20   1.36  -1.15   1.27  -0.20   \n",
              "3      1.01  -1.17  -1.17  -1.15   0.61   0.76   1.27  -0.20   1.36  -1.15   \n",
              "4      1.01  -0.34  -1.17   1.27  -1.17  -0.99  -1.18   0.61   1.27   0.76   \n",
              "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "6257   1.01  -1.47  -0.20  -1.17  -1.47   0.61  -1.47   1.27  -0.20   1.27   \n",
              "6258   1.01  -0.99  -1.17  -0.96   1.50   1.01  -0.20  -0.43  -0.34  -1.18   \n",
              "6259   1.01  -1.15  -1.17  -1.17   0.61  -0.67   1.27  -0.20   1.36  -1.15   \n",
              "6260   1.01  -1.17  -1.17   1.52  -1.18  -0.99   0.61   0.61   1.36  -0.20   \n",
              "6261   1.01  -1.17   1.27   1.36  -0.20   1.52  -1.15   1.27  -0.20   1.27   \n",
              "\n",
              "      ...  13566  13567  13568  13569  13570  13571  13572  13573  13574  \\\n",
              "0     ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "1     ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "2     ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "3     ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "4     ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "6257  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "6258  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "6259  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "6260  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "6261  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "      13575  \n",
              "0       0.0  \n",
              "1       0.0  \n",
              "2       0.0  \n",
              "3       0.0  \n",
              "4       0.0  \n",
              "...     ...  \n",
              "6257    0.0  \n",
              "6258    0.0  \n",
              "6259    0.0  \n",
              "6260    0.0  \n",
              "6261    0.0  \n",
              "\n",
              "[6262 rows x 13576 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flattened_VHSE = []\n",
        "for row in VHSE_encoded:\n",
        "  flattened_VHSE.append(row.flatten('F'))\n",
        "df = pd.DataFrame(flattened_VHSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifiGuvy1EozI"
      },
      "source": [
        "Additionally, these models cannot handle 2D outputs like the CNN can, so the PAM encoding resulted in defining each IUPAC nucleotide code as a separate class, for a total of 15 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pQ2La00tY_X"
      },
      "outputs": [],
      "source": [
        "def encode_nuc(labels):\n",
        "  label_dict = {\n",
        "    'A': 0,\n",
        "    'T': 1,\n",
        "    'C': 2,\n",
        "    'G': 3,\n",
        "    'N': 4,\n",
        "    'R': 5,\n",
        "    'Y': 6,\n",
        "    'M': 7,\n",
        "    'K': 8,\n",
        "    'S': 9,\n",
        "    'W': 10,\n",
        "    'H': 11,\n",
        "    'B': 12,\n",
        "    'V': 13,\n",
        "    'D': 14,\n",
        "  }\n",
        "  ret = []\n",
        "  next = 15\n",
        "  for nuc in labels:\n",
        "      if nuc not in label_dict.keys():\n",
        "        label_dict[nuc] = next\n",
        "        next += 1\n",
        "      ret.append(label_dict[nuc])\n",
        "  print(len(ret))\n",
        "  return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q9NBbgwtat2",
        "outputId": "35e27ed1-6f1e-4d52-8588-ab31d57ab835"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6262\n",
            "6262\n",
            "6262\n",
            "6262\n",
            "6262\n",
            "6262\n",
            "6262\n",
            "6262\n"
          ]
        }
      ],
      "source": [
        "nt1_labels = encode_nuc(nt1)\n",
        "nt2_labels = encode_nuc(nt2)\n",
        "nt3_labels = encode_nuc(nt3)\n",
        "nt4_labels = encode_nuc(nt4)\n",
        "nt5_labels = encode_nuc(nt5)\n",
        "nt6_labels = encode_nuc(nt6)\n",
        "nt7_labels = encode_nuc(nt7)\n",
        "nt8_labels = encode_nuc(nt8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybsq4BdnE3xW"
      },
      "source": [
        "Data for each nucleotide was split with the ratio 67/33 test/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tZ2CBjL1-5Ve"
      },
      "outputs": [],
      "source": [
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(df, nt1_labels, test_size=0.33, random_state = 2)\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(df, nt2_labels, test_size=0.33, random_state = 2)\n",
        "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(df, nt3_labels, test_size=0.33, random_state = 2)\n",
        "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(df, nt4_labels, test_size=0.33, random_state = 2)\n",
        "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(df, nt5_labels, test_size=0.33, random_state = 2)\n",
        "X_train_6, X_test_6, y_train_6, y_test_6 = train_test_split(df, nt6_labels, test_size=0.33, random_state = 2)\n",
        "X_train_7, X_test_7, y_train_7, y_test_7 = train_test_split(df, nt7_labels, test_size=0.33, random_state = 2)\n",
        "X_train_8, X_test_8, y_train_8, y_test_8 = train_test_split(df, nt8_labels, test_size=0.33, random_state = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoKboLZuiB7-"
      },
      "source": [
        "## Logistic Regression\n",
        "Scikit-learn Logistic Regression was trained and tested for each nucleotide position with 100 estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xZrnAeHIgmwh"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_1, y_train_1)\n",
        "test_preds_1 = logReg.predict(X_test_1)\n",
        "train_preds_1 = logReg.predict(X_train_1)\n",
        "print(accuracy_score(y_train_1, train_preds_1), accuracy_score(y_test_1, test_preds_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z5Y-JBr4gmrj"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_2, y_train_2)\n",
        "test_preds_2 = logReg.predict(X_test_2)\n",
        "train_preds_2 = logReg.predict(X_train_2)\n",
        "print(accuracy_score(y_train_2, train_preds_2), accuracy_score(y_test_2, test_preds_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bftVrvPUgml0"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_3, y_train_3)\n",
        "test_preds_3 = logReg.predict(X_test_3)\n",
        "train_preds_3 = logReg.predict(X_train_3)\n",
        "print(accuracy_score(y_train_3, train_preds_3), accuracy_score(y_test_3, test_preds_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RhOdzSNEgmez"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_4, y_train_4)\n",
        "test_preds_4 = logReg.predict(X_test_4)\n",
        "train_preds_4 = logReg.predict(X_train_4)\n",
        "print(accuracy_score(y_train_4, train_preds_4), accuracy_score(y_test_4, test_preds_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wnoot-z1gmPL"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_5, y_train_5)\n",
        "test_preds_5 = logReg.predict(X_test_5)\n",
        "train_preds_5 = logReg.predict(X_train_5)\n",
        "print(accuracy_score(y_train_5, train_preds_5), accuracy_score(y_test_5, test_preds_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vwqj7PPlWZz6"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_6, y_train_6)\n",
        "test_preds_6 = logReg.predict(X_test_6)\n",
        "train_preds_6 = logReg.predict(X_train_6)\n",
        "print(accuracy_score(y_train_6, train_preds_6), accuracy_score(y_test_6, test_preds_6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SWBW_SpggqEv"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_7, y_train_7)\n",
        "test_preds_7 = logReg.predict(X_test_7)\n",
        "train_preds_7 = logReg.predict(X_train_7)\n",
        "print(accuracy_score(y_train_7, train_preds_7), accuracy_score(y_test_7, test_preds_7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WWpR3wDDgqi8"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(random_state=0, max_iter=100).fit(X_train_8, y_train_8)\n",
        "test_preds_8 = logReg.predict(X_test_8)\n",
        "train_preds_8 = logReg.predict(X_train_8)\n",
        "print(accuracy_score(y_train_8, train_preds_8), accuracy_score(y_test_8, test_preds_8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_A9oDwdj_sZ"
      },
      "source": [
        "## SVM\n",
        "Support Vector Machines for each of the 8 nucleotide positions were run with and without grid search for hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGGZzZs9ke2L"
      },
      "source": [
        "Doing Cross Validation Grid Search with k=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0v3bYaj5P5FJ"
      },
      "outputs": [],
      "source": [
        "param_grid = [\n",
        "  {'C': [0.1, 1, 100], 'kernel': ['rbf'], 'gamma': [0.001, 0.0001, 'scale'], 'verbose' : [True]}\n",
        " ]\n",
        "\n",
        "svm = SVC()\n",
        "clf = GridSearchCV(svm, param_grid, cv=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PR7qhv20_8pY"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_1, y_train_1)\n",
        "test_preds_1 = clf.predict(X_test_1)\n",
        "train_preds_1 = clf.predict(X_train_1)\n",
        "print(\"Model 1 Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_1, train_preds_1), accuracy_score(y_test_1, test_preds_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P9712V2qSkXL"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_2, y_train_2)\n",
        "test_preds_2 = clf.predict(X_test_2)\n",
        "train_preds_2 = clf.predict(X_train_2)\n",
        "print(\"Model 2 Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_2, train_preds_2), accuracy_score(y_test_2, test_preds_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aKaVPUaySnyC"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_3, y_train_3)\n",
        "test_preds_3 = clf.predict(X_test_3)\n",
        "train_preds_3 = clf.predict(X_train_3)\n",
        "print(\"Model  Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_3, train_preds_3), accuracy_score(y_test_3, test_preds_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O5jI34HPSojg"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_4, y_train_4)\n",
        "test_preds_4 = clf.predict(X_test_4)\n",
        "train_preds_4 = clf.predict(X_train_4)\n",
        "print(\"Model \" + 4 + \" Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_4, train_preds_4), accuracy_score(y_test_4, test_preds_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vaRoeG4KSo9z"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_5, y_train_5)\n",
        "test_preds_5 = clf.predict(X_test_5)\n",
        "train_preds_5 = clf.predict(X_train_5)\n",
        "print(\"Model \" + 5 + \" Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_5, train_preds_5), accuracy_score(y_test_5, test_preds_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IrfNsWUNSpiO"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_6, y_train_6)\n",
        "test_preds_6 = clf.predict(X_test_6)\n",
        "train_preds_6 = clf.predict(X_train_6)\n",
        "print(\"Model \" + 6 + \" Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_6, train_preds_6), accuracy_score(y_test_6, test_preds_6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uVOE5t6VSqAu"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_7, y_train_7)\n",
        "test_preds_7 = clf.predict(X_test_7)\n",
        "train_preds_7 = clf.predict(X_train_7)\n",
        "print(\"Model \" + 7 + \" Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_7, train_preds_7), accuracy_score(y_test_7, test_preds_7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rU-fU909Sqdn"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_8, y_train_8)\n",
        "test_preds_8 = clf.predict(X_test_8)\n",
        "train_preds_8 = clf.predict(X_train_8)\n",
        "print(\"Model \" + 8 + \" Train, Test Accuracy \\n\")\n",
        "print(accuracy_score(y_train_8, train_preds_8), accuracy_score(y_test_8, test_preds_8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCxyPm2pldtI"
      },
      "source": [
        "Now with no cross validation (just built in SKLearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I9MwCB8Qlj_g"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_1, y_train_1)\n",
        "test_preds_1 = svm.predict(X_test_1)\n",
        "train_preds_1 = svm.predict(X_train_1)\n",
        "print(accuracy_score(y_train_1, train_preds_1), accuracy_score(y_test_1, test_preds_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5LRrX_zTlj_h"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_2, y_train_2)\n",
        "test_preds_2 = svm.predict(X_test_2)\n",
        "train_preds_2 = svm.predict(X_train_2)\n",
        "print(accuracy_score(y_train_2, train_preds_2), accuracy_score(y_test_2, test_preds_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6mOdPa3Alj_h"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_3, y_train_3)\n",
        "test_preds_3 = svm.predict(X_test_3)\n",
        "train_preds_3 = svm.predict(X_train_3)\n",
        "print(accuracy_score(y_train_3, train_preds_3), accuracy_score(y_test_3, test_preds_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FoIURqYnlj_h"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_4, y_train_4)\n",
        "test_preds_4 = svm.predict(X_test_4)\n",
        "train_preds_4 = svm.predict(X_train_4)\n",
        "print(accuracy_score(y_train_4, train_preds_4), accuracy_score(y_test_4, test_preds_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6fojmi6hlj_h"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_5, y_train_5)\n",
        "test_preds_5 = svm.predict(X_test_5)\n",
        "train_preds_5 = svm.predict(X_train_5)\n",
        "print(accuracy_score(y_train_5, train_preds_5), accuracy_score(y_test_5, test_preds_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8fCARu50lj_h"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_6, y_train_6)\n",
        "test_preds_6 = svm.predict(X_test_6)\n",
        "train_preds_6 = svm.predict(X_train_6)\n",
        "print(accuracy_score(y_train_6, train_preds_6), accuracy_score(y_test_6, test_preds_6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RdkPW5eElj_h"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_7, y_train_7)\n",
        "test_preds_7 = svm.predict(X_test_7)\n",
        "train_preds_7 = svm.predict(X_train_7)\n",
        "print(accuracy_score(y_train_7, train_preds_7), accuracy_score(y_test_7, test_preds_7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8KRQZHKBlj_h"
      },
      "outputs": [],
      "source": [
        "svm.fit(X_train_8, y_train_8)\n",
        "test_preds_8 = svm.predict(X_test_8)\n",
        "train_preds_8 = svm.predict(X_train_8)\n",
        "print(accuracy_score(y_train_8, train_preds_8), accuracy_score(y_test_8, test_preds_8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "behlv0Iqk5iP"
      },
      "source": [
        "## AdaBoost\n",
        "Sci-kit learn AdaBoost Classifier was run for all 8 nucleotide positions with 200 estimators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SBEmWKx5p-jQ"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_1, y_test_1)\n",
        "test_preds_1 = AdaBoost.predict(X_test_1)\n",
        "train_preds_1 = AdaBoost.predict(X_train_1)\n",
        "print(accuracy_score(y_train_1, train_preds_1), accuracy_score(y_test_1, test_preds_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wIjKvW4Sp-jR"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_2, y_test_2)\n",
        "test_preds = AdaBoost.predict(X_test_2)\n",
        "train_preds = AdaBoost.predict(X_train_2)\n",
        "print(accuracy_score(y_train_2, train_preds), accuracy_score(y_test_2, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cSTuvk1dp-jR"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_3, y_test_3)\n",
        "test_preds = AdaBoost.predict(X_test_3)\n",
        "train_preds = AdaBoost.predict(X_train_3)\n",
        "print(accuracy_score(y_train_3, train_preds), accuracy_score(y_test_3, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dzi5nUWop-jR"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_4, y_test_4)\n",
        "test_preds = AdaBoost.predict(X_test_4)\n",
        "train_preds = AdaBoost.predict(X_train_4)\n",
        "print(accuracy_score(y_train_4, train_preds), accuracy_score(y_test_4, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XY2AouGcp-jR"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_5, y_test_5)\n",
        "test_preds = AdaBoost.predict(X_test_5)\n",
        "train_preds = AdaBoost.predict(X_train_5)\n",
        "print(accuracy_score(y_train_5, train_preds), accuracy_score(y_test_5, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8EA1j6eFp-jR"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_6, y_test_6)\n",
        "test_preds = AdaBoost.predict(X_test_6)\n",
        "train_preds = AdaBoost.predict(X_train_6)\n",
        "print(accuracy_score(y_train_6, train_preds), accuracy_score(y_test_6, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DnDvvScqp-jR"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_7, y_test_7)\n",
        "test_preds = AdaBoost.predict(X_test_7)\n",
        "train_preds = AdaBoost.predict(X_train_7)\n",
        "print(accuracy_score(y_train_7, train_preds), accuracy_score(y_test_7, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "57hzw2D5p-jS"
      },
      "outputs": [],
      "source": [
        "AdaBoost = AdaBoostClassifier(n_estimators=200, random_state=0)\n",
        "AdaBoost.fit(X_test_8, y_test_8)\n",
        "test_preds = AdaBoost.predict(X_test_8)\n",
        "train_preds = AdaBoost.predict(X_train_8)\n",
        "print(accuracy_score(y_train_8, train_preds), accuracy_score(y_test_8, test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKHVwOpXpag_"
      },
      "source": [
        "#Trying ANKH Encoding\n",
        "This simply represents an attempt to embed the protein sequences using Ankh, this code does not execute without errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WZt8dVSPpag_"
      },
      "outputs": [],
      "source": [
        "!pip install ankh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hp0D6tdvpag_"
      },
      "outputs": [],
      "source": [
        "  import ankh\n",
        "\n",
        "  # To load large model:\n",
        "  model, tokenizer = ankh.load_large_model()\n",
        "  model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ARJgxrSSpag_"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRGm-C6Vpag_"
      },
      "source": [
        "feature extraction using large model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VljfiVvopahA"
      },
      "outputs": [],
      "source": [
        "  model, tokenizer = ankh.load_large_model()\n",
        "  model.eval()\n",
        "\n",
        "  protein_sequences = protseqs\n",
        "\n",
        "  protein_sequences = [list(seq) for seq in protein_sequences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FX5Km4yQpahA"
      },
      "outputs": [],
      "source": [
        "type(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O6kyND-6pahA"
      },
      "outputs": [],
      "source": [
        "outputs = tokenizer.batch_encode_plus(protein_sequences, \n",
        "                                    add_special_tokens=True, \n",
        "                                    padding=True, \n",
        "                                    is_split_into_words=True, \n",
        "                                    return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcAAhBa8pahA"
      },
      "source": [
        "load model and number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8OapjcTupahA"
      },
      "outputs": [],
      "source": [
        "def get_num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L0efOYqbpahA"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import ankh\n",
        "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
        "from datasets import load_dataset\n",
        "import transformers.models.convbert as c_bert\n",
        "from scipy import stats\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7GQf38zqpahA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AqrnTwgrpahA"
      },
      "outputs": [],
      "source": [
        "#Select the available device.\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cuda')\n",
        "print('Available device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MYXFkhs1pahA"
      },
      "outputs": [],
      "source": [
        "# imports are always needed\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YCIpmPYHpahB"
      },
      "outputs": [],
      "source": [
        "# get index of currently selected device\n",
        "torch.cuda.current_device() # returns 0 in my case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0cXhPKkKpahB"
      },
      "outputs": [],
      "source": [
        "# get number of GPUs available\n",
        "torch.cuda.device_count() # returns 1 in my case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M745fIf3pahB"
      },
      "outputs": [],
      "source": [
        "# get the name of the device\n",
        "torch.cuda.get_device_name(0) # good old Tesla K80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MxGAVFV1pahB"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = ankh.load_large_model()\n",
        "model.eval()\n",
        "model.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5PyBk9YIpahB"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of parameters:\", get_num_params(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH9NgVwKpahB"
      },
      "source": [
        "Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pkAijSCLpahB"
      },
      "outputs": [],
      "source": [
        "data8 = pd.read_csv('/content/drive/My Drive/team_5/project_1/Data/PAM_data8_ranked.csv')\n",
        "data_8nt = pd.DataFrame()\n",
        "data_8nt['AA Sequence'] = data8['Sequence']\n",
        "data_8nt['Sequence Length'] = data8['Length']\n",
        "data_8nt['PAM'] = data8['consensus PAM']\n",
        "\n",
        "data_8nt['nt1'] = data_8nt.PAM.str.split('',expand=True)[1]\n",
        "data_8nt['nt2'] = data_8nt.PAM.str.split('',expand=True)[2]\n",
        "data_8nt['nt3'] = data_8nt.PAM.str.split('',expand=True)[3]\n",
        "data_8nt['nt4'] = data_8nt.PAM.str.split('',expand=True)[4]\n",
        "data_8nt['nt5'] = data_8nt.PAM.str.split('',expand=True)[5]\n",
        "data_8nt['nt6'] = data_8nt.PAM.str.split('',expand=True)[6]\n",
        "data_8nt['nt7'] = data_8nt.PAM.str.split('',expand=True)[7]\n",
        "data_8nt['nt8'] = data_8nt.PAM.str.split('',expand=True)[8]\n",
        "\n",
        "data_8nt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydChZFwfpahB"
      },
      "source": [
        "View dataset from the ankh github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O2fwPCVSpahC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"proteinea/Fluorosence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6kIfOsgfpahC"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oLvVPuHCpahC"
      },
      "outputs": [],
      "source": [
        "dataset['train']['log_fluorescence']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJUhteXGpahC"
      },
      "source": [
        "split data in train,test,validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LBOgzImopahC"
      },
      "outputs": [],
      "source": [
        "aa_data = data_8nt['AA Sequence']\n",
        "pam_data = data_8nt['PAM']\n",
        "nt1 = data_8nt['nt1']\n",
        "nt2 = data_8nt['nt2']\n",
        "nt3 = data_8nt['nt3']\n",
        "nt4 = data_8nt['nt4']\n",
        "nt5 = data_8nt['nt5']\n",
        "nt6 = data_8nt['nt6']\n",
        "nt7 = data_8nt['nt7']\n",
        "nt8 = data_8nt['nt8']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ppFNEcRSpahC"
      },
      "outputs": [],
      "source": [
        "nt1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NwHa2cPEpahC"
      },
      "outputs": [],
      "source": [
        "# full pam sequence --> taking insanely long for embedding (23 hrs)\n",
        "train_seq, test_seq, train_pam, test_pam = train_test_split(aa_data,pam_data, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_pam, val_pam = train_test_split(aa_data, pam_data, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TCKy5ocBpahC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b_2abRhmpahC"
      },
      "outputs": [],
      "source": [
        "# Get the mean of the labels to initialize \n",
        "# the final layer's bias with it for faster convergence in regression tasks.\n",
        "training_labels_mean = np.mean(pam_data)\n",
        "training_labels_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uZwspwVMpahC"
      },
      "outputs": [],
      "source": [
        "# each individual nucleotide (in the pam)\n",
        "train_seq, test_seq, train_nt1, test_nt1 = train_test_split(aa_data,nt1_encoded, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt1, val_nt1 = train_test_split(train_seq, train_nt1, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7TIxeXyppahD"
      },
      "outputs": [],
      "source": [
        "train_seq, test_seq, train_nt2, test_nt2 = train_test_split(aa_data,nt2, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt2, val_nt2 = train_test_split(train_seq, train_nt2, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M1edv9_ipahD"
      },
      "outputs": [],
      "source": [
        "train_seq, test_seq, train_nt3, test_nt3 = train_test_split(aa_data,nt3, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt3, val_nt3 = train_test_split(train_seq, train_nt3, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ExyqTVtHpahD"
      },
      "outputs": [],
      "source": [
        "train_seq, test_seq, train_nt4, test_nt4 = train_test_split(aa_data,nt4, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt4, val_nt4 = train_test_split(train_seq, train_nt4, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SUzOJWKlpahD"
      },
      "outputs": [],
      "source": [
        "train_seq, test_seq, train_nt5, test_nt5 = train_test_split(aa_data,nt5, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt5, val_nt5 = train_test_split(train_seq, train_nt5, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H04S68QxpahD"
      },
      "outputs": [],
      "source": [
        "train_seq, test_seq, train_nt6, test_nt6 = train_test_split(aa_data,nt6, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt6, val_nt6 = train_test_split(train_seq, train_nt6, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V8TDRuFhpahD"
      },
      "outputs": [],
      "source": [
        "train_seq, test_seq, train_nt7, test_nt7 = train_test_split(aa_data,nt7, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt7, val_nt7 = train_test_split(train_seq, train_nt7, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dCwzmwvapahD"
      },
      "outputs": [],
      "source": [
        "train_seq, test_seq, train_nt8, test_nt8 = train_test_split(aa_data,nt8, test_size=0.2, random_state = 2)\n",
        "train_seq, val_seq, train_nt8, val_nt8 = train_test_split(train_seq, train_nt8, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M36IN8UpahD"
      },
      "source": [
        "preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fe2eNHhfpahD"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(sequences, labels, max_length=None):\n",
        "    '''\n",
        "        Args:\n",
        "            sequences: list, the list which contains the protein primary sequences.\n",
        "            max_length, Integer, the maximum sequence length, \n",
        "            if there is a sequence that is larger than the specified sequence length will be post-truncated. \n",
        "    '''\n",
        "    if max_length is None:\n",
        "        max_length = len(max(train_seq, key=lambda x: len(x)))\n",
        "    splitted_sequences = [list(seq[:max_length]) for seq in sequences]\n",
        "    return splitted_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "al2nj0pQpahD"
      },
      "outputs": [],
      "source": [
        "# train_seq\n",
        "train_seq_1 = preprocess_dataset(train_seq, train_nt1)\n",
        "train_seq_2 = preprocess_dataset(train_seq, train_nt2)\n",
        "train_seq_3 = preprocess_dataset(train_seq, train_nt3)\n",
        "train_seq_4 = preprocess_dataset(train_seq, train_nt4)\n",
        "train_seq_5 = preprocess_dataset(train_seq, train_nt5)\n",
        "train_seq_6 = preprocess_dataset(train_seq, train_nt6)\n",
        "train_seq_7 = preprocess_dataset(train_seq, train_nt7)\n",
        "train_seq_8 = preprocess_dataset(train_seq, train_nt8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I7yaJXNupahD"
      },
      "outputs": [],
      "source": [
        "# test_seq\n",
        "test_seq_1 = preprocess_dataset(test_seq, test_nt1)\n",
        "test_seq_2 = preprocess_dataset(test_seq, test_nt2)\n",
        "test_seq_3 = preprocess_dataset(test_seq, test_nt3)\n",
        "test_seq_4 = preprocess_dataset(test_seq, test_nt4)\n",
        "test_seq_5 = preprocess_dataset(test_seq, test_nt5)\n",
        "test_seq_6 = preprocess_dataset(test_seq, test_nt6)\n",
        "test_seq_7 = preprocess_dataset(test_seq, test_nt7)\n",
        "test_seq_8 = preprocess_dataset(test_seq, test_nt8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7y5ZL44CpahD"
      },
      "outputs": [],
      "source": [
        "# val_seq\n",
        "val_seq_1 = preprocess_dataset(val_seq, val_nt1)\n",
        "val_seq_2 = preprocess_dataset(val_seq, val_nt2)\n",
        "val_seq_3 = preprocess_dataset(val_seq, val_nt3)\n",
        "val_seq_4 = preprocess_dataset(val_seq, val_nt4)\n",
        "val_seq_5 = preprocess_dataset(val_seq, val_nt5)\n",
        "val_seq_6 = preprocess_dataset(val_seq, val_nt6)\n",
        "val_seq_7 = preprocess_dataset(val_seq, val_nt7)\n",
        "val_seq_8 = preprocess_dataset(val_seq, val_nt8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9BphsNMYpahD"
      },
      "outputs": [],
      "source": [
        "# full pam sequence --> taking insanely long for embedding (23 hrs) -- so is train_seq by nucleotide\n",
        "train_seq = preprocess_dataset(train_seq, train_pam)\n",
        "test_seq = preprocess_dataset(test_seq, test_pam)\n",
        "val_seq = preprocess_dataset(val_seq, val_pam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fKuK8Z2pahD"
      },
      "source": [
        "extract sequences embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hOwILIPlpahD"
      },
      "outputs": [],
      "source": [
        "def embed_dataset(model, sequences, shift_left = 0, shift_right = -1):\n",
        "    inputs_embedding = []\n",
        "    with torch.no_grad():\n",
        "        for sample in tqdm(sequences):\n",
        "            ids = tokenizer.batch_encode_plus([sample], add_special_tokens=True, \n",
        "                                              padding=True, is_split_into_words=True, \n",
        "                                              return_tensors=\"pt\")\n",
        "            embedding = model(input_ids=ids['input_ids'].to(device))[0]\n",
        "            embedding = embedding[0].detach().cpu().numpy()[shift_left:shift_right]\n",
        "            inputs_embedding.append(embedding)\n",
        "    return inputs_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sUG7gmbepahE"
      },
      "outputs": [],
      "source": [
        "training_embeddings = embed_dataset(model, train_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SLvDjx7cpahE"
      },
      "outputs": [],
      "source": [
        "validation_embeddings = embed_dataset(model, val_seq)\n",
        "test_embeddings = embed_dataset(model, test_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz4BCLrYpahE"
      },
      "source": [
        "sequence embeddings with pam (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2qwbf0aXpahE"
      },
      "outputs": [],
      "source": [
        "class pamdataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "        return {\n",
        "            'embed': torch.tensor(sample),\n",
        "            'labels': torch.tensor(label, dtype=torch.float32).unsqueeze(-1)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SBwcc4utpahE"
      },
      "outputs": [],
      "source": [
        "training_dataset = pamdataset(training_embeddings, train_pam)\n",
        "validation_dataset = pamdataset(validation_embeddings, val_pam)\n",
        "test_ds = pamdataset(test_embeddings, test_pam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TcGl0R7spahE"
      },
      "outputs": [],
      "source": [
        "training_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZB3rNwZpahE"
      },
      "source": [
        "downstream multiclass classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-eUhqiNQpahE"
      },
      "outputs": [],
      "source": [
        "def model_init(embed_dim, training_labels_mean=None):\n",
        "    hidden_dim = int(embed_dim / 2)\n",
        "    num_hidden_layers = 1\n",
        "    nlayers = 1\n",
        "    nhead = 4\n",
        "    dropout = 0.2\n",
        "    conv_kernel_size = 7\n",
        "    pooling = 'max' # available pooling methods ['avg', 'max']\n",
        "    downstream_model = ankh.ConvBertForMultiClassClassification(input_dim=embed_dim, \n",
        "                                                  nhead=nhead, \n",
        "                                                  hidden_dim=hidden_dim, \n",
        "                                                  num_hidden_layers=num_hidden_layers, \n",
        "                                                  num_layers=nlayers, \n",
        "                                                  kernel_size=conv_kernel_size,\n",
        "                                                  dropout=dropout, \n",
        "                                                  pooling=pooling, \n",
        "                                                  training_labels_mean=training_labels_mean)\n",
        "    return downstream_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7iaPa8InpahE"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    return {\n",
        "        \"spearmanr\": stats.spearmanr(p.label_ids, p.predictions).correlation,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EeU25bGfpahE"
      },
      "outputs": [],
      "source": [
        "type(protseqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RkidtvPSpahE"
      },
      "outputs": [],
      "source": [
        "model_type = 'ankh_large'\n",
        "experiment = f'flu_{model_type}'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'./results_{experiment}',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    warmup_steps=1000,\n",
        "    learning_rate=1e-03,\n",
        "    weight_decay=0.0,\n",
        "    logging_dir=f'./logs_{experiment}',\n",
        "    logging_steps=200,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=16,\n",
        "    fp16=False,\n",
        "    fp16_opt_level=\"02\",\n",
        "    run_name=experiment,\n",
        "    seed=42,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_spearmanr\",\n",
        "    greater_is_better=True,\n",
        "    save_strategy=\"epoch\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KuF9cRZypahE"
      },
      "outputs": [],
      "source": [
        "model_embed_dim = 1536\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=partial(model_init, embed_dim=model_embed_dim),\n",
        "    args=training_args,\n",
        "    train_dataset=training_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kB2W8ehrpahE"
      },
      "outputs": [],
      "source": [
        "model_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tHw78AJFpahE"
      },
      "outputs": [],
      "source": [
        "model_embed_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zde64rwppahE"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEOtfkOVpahE"
      },
      "source": [
        "### POSITIONAL ENCODING (COS) + Embedding -- RUN ANKH MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISJLzOCopahF"
      },
      "source": [
        "The formula for calculating the positional encoding (implemented in Python below) is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tLAAnryhpahF"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "  \n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwk8jeqnpahF"
      },
      "source": [
        "The position encoding function is a stack of sines and cosines that vibrate at different frequencies depending on their location along the depth of the embedding vector. They vibrate across the position axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cWmGbe1SpahF"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "pos_encoding = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "# Check the shape.\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "# Plot the dimensions.\n",
        "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HNSRFibpahF"
      },
      "source": [
        "By definition these vectors align well with nearby vectors along the position axis. Below the position encoding vectors are normalized and the vector from position `1000` is compared, by dot-product, to all the others:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7hYOSTzIpahF"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n",
        "p = pos_encoding[1000]\n",
        "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(dots)\n",
        "plt.ylim([0,1])\n",
        "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
        "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
        "plt.legend()\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(dots)\n",
        "plt.xlim([950, 1050])\n",
        "plt.ylim([0,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yf0QJoWrpahF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MuzYE4_hfBQ9",
        "3BZtV6xfIvaY",
        "8fMk0IDSfUVN",
        "7mX0Le4qh4Sv",
        "MoKboLZuiB7-",
        "g_A9oDwdj_sZ",
        "behlv0Iqk5iP",
        "uKHVwOpXpag_"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}